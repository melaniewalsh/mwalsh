{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Life and Anatomy of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the nitty-gritty of the Python programming language, we're going to discuss what Python code looks like, how you can write and run it, and what you can do with it.\n",
    "\n",
    "The first few times that I tried to learn Python â€” yes, it took me multiple tries! â€” it felt like learning a bunch of made-up rules about an imaginary universe. Turns out Python *is* kind of like an imaginary universe with made-up rules, and that's part of what makes it fun. But it can also make learning Python difficult if you don't really know what the imaginary universe looks like, or how it functions, or how it relates to your universe and your specific goals â€” such as doing text analysis or making a Twitter bot or creating a network visualization.\n",
    "\n",
    "We're going to demonstrate what Python looks like in action, so you can get a feel for its structure and flow. Don't get too bogged down in the details for now. Just try to get a sense â€” at an abstract level â€” of how Python works and how you might use it.\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2017/01/31/23/21/animal-2028134_960_720.png\" alt=\"The command line\" width=\"100%\">\n",
    "\n",
    "# Example Python Code â€” Counting Words in a Text File\n",
    "\n",
    "Below is a chunk of Python code. These lines, when put together, do something simple yet important. They count and display the most frequent words in a text file. The example below specifically counts and displays the 40 most frequent words in Charlotte Perkins Gilman's short story \"The Yellow Wallpaper\" (1892)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melaniewalsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15),\n",
       " ('know', 15),\n",
       " ('go', 15),\n",
       " ('really', 14),\n",
       " ('thing', 14),\n",
       " ('wallpaper', 13),\n",
       " ('night', 13),\n",
       " ('long', 12),\n",
       " ('course', 12),\n",
       " ('things', 12),\n",
       " ('take', 12),\n",
       " ('always', 12),\n",
       " ('could', 12),\n",
       " ('jennie', 12),\n",
       " ('great', 11),\n",
       " ('says', 11),\n",
       " ('feel', 11),\n",
       " ('even', 11),\n",
       " ('used', 11),\n",
       " ('dear', 11),\n",
       " ('time', 11),\n",
       " ('enough', 11),\n",
       " ('away', 11),\n",
       " ('want', 11),\n",
       " ('never', 10),\n",
       " ('must', 10)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
    "nltk_stop_words = stopwords.words(\"english\")\n",
    "number_of_desired_words = 40\n",
    "\n",
    "full_text = open(filepath_of_text).read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you could have guessed these metrics without the help of a computer. Maybe not. Maybe you could make an interesting argument about the story with these metrics. Maybe not.\n",
    "\n",
    "Calculating word frequency is a very basic form of computational text analysis and often not terribly interesting on its own, especially with a single short text. However, calculating word frequency *is* important, and it's at the center of most text analysis approaches, even far more complicated ones. That's why we're starting with it.\n",
    "\n",
    "> ðŸ—£ï¸Heads up! This is just *one* way to count words in a text file, not the one *right* way. There is no *right* way to count words in a text file or to do anything else in Python.\n",
    "\n",
    ">Rather than asking \"Is this code *right*?\", you want to ask:\n",
    ">- \"Is this code *efficient*?\"\n",
    ">- \"Is this code *readable*?\"\n",
    ">- \"Does this code *help me accomplish my goal*?\"\n",
    "\n",
    ">Sometimes you'll prioritize one concern over another. Maybe your code isn't as efficient as humanly possible, but if it gets the job done, and you understand it, then you might not care about maximum efficiency. I wrote the code above to help illustrate a number of key Python concepts, but there are many other ways I could have written it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Anatomy of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries/Packages/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for some great Python news? You don't have to code everything by yourself from scratch! Many other people have written Python code that you can `import` into your own code, which will save you time and often do a lot of complex, powerful work behind-the-scenes. \n",
    "\n",
    "We call the code written and packaged up by other people a \"library,\" \"package,\" or \"module.\" We'll talk more about them in a couple weeks. For now simply know that you `import` libraries/packages/modules at the very top of a Python script for later use.\n",
    "\n",
    "- [`Counter`](https://stackabuse.com/introduction-to-pythons-collections-module/) will help me count words\n",
    "- `re`, short for regular expressions, is basically a fancy find-and-replace that will help me split \"The Yellow Wallpaper\" into individual words and get rid of trailing punctuation\n",
    "- [`nltk`](https://spacy.io/usage/spacy-101#whats-spacy), a natural language processing library, will give me a list of \"stopwords\" (but it can do a *lot* more than that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(any_chunk_of_text):\n",
    "    words = re.split(\"\\W+\", any_chunk_of_text.lower())\n",
    "    return words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `import`ing modules and libraries, you typically `def`ine your \"functions.\" Functions are a nifty way to bundle up code so that you can use them again later. Functions also keep your code neat and tidy.\n",
    "\n",
    "Here we're making a function called `split_into_words`, which takes in any chunk of text, transforms that text to lower-case, and splits the text into a list of clean words without punctuation or spaces. We're not actually using the function yet. We're just setting it up. We'll talk more about functions in two weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Filepaths and Assign Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
    "nltk_stop_words = stopwords.words(\"english\")\n",
    "number_of_desired_words = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we establish some values and data that we're going to call later. We'll need the filepath of the short story in order to read it, so we make a variable called `filepath_of_text` and assign it to the relative path of the text file `\"../texts/literature/The-Yellow-Wallpaper.txt\"`. We also make a variable called `nltk_stop_words` and plug in the stopwords from the `nltk` library. Lastly we make a variable called `number_of_desired_words`, which will eventually tell the script how many words to display, and we assign it the value `40`. If we changed this value to `50`, then the script would display 50 words instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = open(filepath_of_text).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above opens Charlotte Perkins Gilman's \"The Yellow Wallpaper,\" reads in the novel, and then assigns it to the variable `full_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate and Analyze File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_words = split_into_words(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the words in \"The Yellow Wallpaper,\" we need to break the full text into individual words. Above  we call the function `split_into_words`, which we created earlier, and use it to split the `full_text` of the story into individual words. Then we put the words inside a variable called `all_the_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we don't really care about tiny words â€” such as \"the,\" \"and,\" \"or,\" â€” so we're going to remove them from our list. The line of code above makes a new list of words that includes every word in `all_the_words` if it does *not* appear in `nltk_stop_words` (aka it nixes the stopwords). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_words_tally = Counter(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to count! We plug `meaningful_words` into our `Counter`, which gives us a tally of how many times each word in the story appears. Then we put the tally into a new variable called `meaningful_words_tally`. This kind of tally is called a \"dictionary,\" which we will discuss two weeks from now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we pull out the top 40 most frequently occurring words from our complete tally. We make one final variable and grab our top `number_of_desired_words`, which we previously established as \"40.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('john', 45), ('one', 33), ('said', 30), ('would', 27), ('get', 24), ('see', 24), ('room', 24), ('pattern', 24), ('paper', 23), ('like', 21), ('little', 20), ('much', 16), ('good', 16), ('think', 16), ('well', 15), ('know', 15), ('go', 15), ('really', 14), ('thing', 14), ('wallpaper', 13), ('night', 13), ('long', 12), ('course', 12), ('things', 12), ('take', 12), ('always', 12), ('could', 12), ('jennie', 12), ('great', 11), ('says', 11), ('feel', 11), ('even', 11), ('used', 11), ('dear', 11), ('time', 11), ('enough', 11), ('away', 11), ('want', 11), ('never', 10), ('must', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(most_frequent_meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we display the results with `print()`, a built-in Python function for displaying data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the script again with explanations. These are called comments. Lines that begin with a hash symbol `#` are excluded from the running code, so you can use them to write notes or instructions to yourself or others. If you want to write a multi-line comment, you can put it in between three quotations marks `\"\"\" \"\"\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melaniewalsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15),\n",
       " ('know', 15),\n",
       " ('go', 15),\n",
       " ('really', 14),\n",
       " ('thing', 14),\n",
       " ('wallpaper', 13),\n",
       " ('night', 13),\n",
       " ('long', 12),\n",
       " ('course', 12),\n",
       " ('things', 12),\n",
       " ('take', 12),\n",
       " ('always', 12),\n",
       " ('could', 12),\n",
       " ('jennie', 12),\n",
       " ('great', 11),\n",
       " ('says', 11),\n",
       " ('feel', 11),\n",
       " ('even', 11),\n",
       " ('used', 11),\n",
       " ('dear', 11),\n",
       " ('time', 11),\n",
       " ('enough', 11),\n",
       " ('away', 11),\n",
       " ('want', 11),\n",
       " ('never', 10),\n",
       " ('must', 10)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Python code for\n",
    "calculating word frequency\n",
    "in a text file\n",
    "\"\"\"\n",
    "\n",
    "#Import Libraries and Modules\n",
    "\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define Functions\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "# Define Filepaths and Assign Variables\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
    "nltk_stop_words = stopwords.words(\"english\")\n",
    "number_of_desired_words = 40\n",
    "\n",
    "# Read in File\n",
    "\n",
    "full_text = open(filepath_of_text).read()\n",
    "\n",
    "# Manipulate and Analyze File\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "# Output Results\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Life of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook / JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary way that we're going to write and run Python in this class is through JupyterLab and Jupyter notebooks. As we've already covered, Jupyter notebooks are documents that can combine live code, explanatory text, and nice displays of data, which makes them great for teaching and learning. But it's also a fully functional way to run Python. By running a cell of Python code in a Jupyter notebook, you can read files from your computer and write files to your computer, you can make and save a bar chart, you can gather data from YouTube and Spotify, you can programmatically tweet from a Twitter bot account, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
    "nltk_stop_words = stopwords.words(\"english\")\n",
    "number_of_desired_words = 40\n",
    "\n",
    "full_text = open(filepath_of_text).read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "with open(\"most-frequent-words-Yellow-Wallpaper.txt\", \"w\") as file_object:\n",
    "    file_object.write(str(most_frequent_meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding two lines of code at the bottom of our script, I can output the most frequent words from \"The Yellow Wallpaper\" into a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Editor â€”> Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run a Python script by writing it in a text editor and then running it from the command line.\n",
    "\n",
    "If you copy and paste the code above into a simple text editor (like TextEdit or NotePad) and name the file with the extension \".py\" (the file extension for Python code), you should be able to run the script from your command line.\n",
    "\n",
    "<img src=\"../images/Python-plain-text.png\" width=100%>\n",
    "\n",
    "All you need to do is call `python` with the name of the Python file (and also make sure that the script includes the correct file path location for the short story file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('john', 45), ('one', 33), ('said', 30), ('would', 27), ('get', 24), ('see', 24), ('room', 24), ('pattern', 24), ('paper', 23), ('like', 21), ('little', 20), ('much', 16), ('good', 16), ('think', 16), ('well', 15), ('know', 15), ('go', 15), ('really', 14), ('thing', 14), ('wallpaper', 13), ('night', 13), ('long', 12), ('course', 12), ('things', 12), ('take', 12), ('always', 12), ('could', 12), ('jennie', 12), ('great', 11), ('says', 11), ('feel', 11), ('even', 11), ('used', 11), ('dear', 11), ('time', 11), ('enough', 11), ('away', 11), ('want', 11), ('never', 10), ('must', 10)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency_Yellow_Wallpaper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Python-Atom.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's possible to write Python from TextEdit, it's not very common, because it's a pain. It's much more common to write Python code in a text editor like Atom, as shown above. You can see that there's all sorts of formatting and functionality that makes the code writing faster and easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write Python scripts such that they can work with different files or any file you want it to. With a few small alterations, our word frequency script can crunch numbers for Grimms Fairy Tales..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 911), ('little', 498), ('one', 454), ('king', 438), ('went', 385), ('came', 359), ('go', 266), ('away', 239), ('old', 233), ('man', 225), ('good', 211), ('took', 210), ('two', 209), ('woman', 199), ('saw', 193), ('could', 193), ('come', 186), ('time', 184), ('day', 180), ('would', 178), ('well', 177), ('_', 163), ('home', 163), ('back', 161), ('shall', 159), ('eyes', 157), ('three', 153), ('daughter', 150), ('mother', 145), ('house', 142), ('thought', 139), ('must', 139), ('forest', 138), ('great', 136), ('cried', 134), ('take', 134), ('long', 133), ('door', 132), ('nothing', 130), ('let', 130)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency.py ../texts/literature/Grimms-Fairy-Tales.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or Louisa May Alcott's *Little Women*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jo', 1416), ('one', 903), ('said', 841), ('little', 773), ('meg', 704), ('amy', 670), ('laurie', 615), ('like', 607), ('beth', 496), ('good', 480), ('would', 441), ('see', 425), ('go', 402), ('old', 396), ('mother', 388), ('much', 377), ('never', 375), ('well', 372), ('could', 360), ('away', 343), ('mr', 334), ('time', 332), ('march', 326), ('know', 322), ('made', 305), ('home', 288), ('young', 286), ('girls', 282), ('come', 280), ('day', 280), ('think', 280), ('say', 277), ('came', 275), ('went', 275), ('dear', 266), ('face', 265), ('got', 260), ('make', 258), ('mrs', 257), ('asked', 256)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency.py ../texts/literature/Little-Women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or any other text your heart desires!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
